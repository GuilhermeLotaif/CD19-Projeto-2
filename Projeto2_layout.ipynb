{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "# &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;                        Projeto 2 - Ciência dos Dados\n",
    "### &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;                 Alexandre Cury, Guilherme Lotaif"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## 1. Problema: Classificador automático de sentimento\n",
    "<br>\n",
    "\n",
    "O Classificador que este projeto terá como base é o Naive-Bayes, que essencialmente irá determinar se um pedaço de texto é relevante ou não a um assunto especifico. Iremos trabalhar com posts coletados da rede social Twitter, em forma de tweets, estes serão divididos em dois grupos: Treinamento e teste, para que possamos fazer com que nosso classificador apreenda como discernir o conteudo dos tweets.\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparando o ambiente no jupyter:\n",
    "\n",
    "Para fazer uma análise e classificação dos nossos dados, precisamos primeiro coletar os tweets do assunto escolhido em um arquivo excel, que será usado como nossa base de dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "#Instalando o tweepy\n",
    "!pip install tweepy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Importando as bibliotecas necessárias:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from random import shuffle\n",
    "import pandas as pd\n",
    "import os.path\n",
    "import tweepy\n",
    "import math\n",
    "import json\n",
    "import sys\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autenticando no  Twitter\n",
    "\n",
    "Tendo em vista que precisamos da permissão do twitter para acessar e coletar os tweets, é necessário ter uma conta de desenvolvedor, além de solicitar as chaves de acesso. Para conseguir estas, acompanhe o [tutorial](https://developer.twitter.com/en/docs/basics/authentication/guides/access-tokens.html) o próprio twitter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#Dados de autenticação do twitter:\n",
    "#leitura do arquivo no formato JSON\n",
    "with open('auth.pass') as fp:    \n",
    "    data = json.load(fp)\n",
    "\n",
    "#Configurando a biblioteca. Não modificar\n",
    "auth = tweepy.OAuthHandler(data['consumer_key'], data['consumer_secret'])\n",
    "auth.set_access_token(data['access_token'], data['access_token_secret'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Escolha do produto e coleta das mensagens\n",
    "\n",
    "Agora é feita a seleção do produto que iremos análisar, e para coletar os tweets para o nosso projeto, é preciso especificar a quantidade de posts que serão importados `(quanto maior esta quantidade for, mais preciso será o nosso classificador)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#Produto escolhido:\n",
    "produto = 'Tesla model s'\n",
    "#Quantidade mínima de mensagens capturadas:\n",
    "n = 500\n",
    "#Quantidade mínima de mensagens para a base de treinamento:\n",
    "t = 300\n",
    "#Filtro de língua, escolha uma na tabela ISO 639-1.\n",
    "lang = 'en'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Capturando os dados do twitter:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#Cria um objeto para a captura\n",
    "api = tweepy.API(auth)\n",
    "#Inicia a captura, para mais detalhes: ver a documentação do tweepy\n",
    "i = 1\n",
    "msgs = []\n",
    "for msg in tweepy.Cursor(api.search, q=produto, lang=lang, tweet_mode=\"extended\").items():    \n",
    "    msgs.append(msg.full_text.lower())\n",
    "    i += 1\n",
    "    if i > n:\n",
    "        break\n",
    "\n",
    "#Embaralhando as mensagens para reduzir um possível viés\n",
    "shuffle(msgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Salvando os dados em uma planilha Excel:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#Verifica se o arquivo não existe para não substituir um conjunto pronto\n",
    "if not os.path.isfile('./{0}.xlsx'.format(produto)):\n",
    "    \n",
    "    #Abre o arquivo para escrita\n",
    "    writer = pd.ExcelWriter('{0}.xlsx'.format(produto))\n",
    "\n",
    "    #divide o conjunto de mensagens em duas planilhas\n",
    "    dft = pd.DataFrame({'Treinamento' : pd.Series(msgs[:t])})\n",
    "    dft.to_excel(excel_writer = writer, sheet_name = 'Treinamento', index = False)\n",
    "\n",
    "    dfc = pd.DataFrame({'Teste' : pd.Series(msgs[t:])})\n",
    "    dfc.to_excel(excel_writer = writer, sheet_name = 'Teste', index = False)\n",
    "\n",
    "    #fecha o arquivo\n",
    "    writer.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### Importando e limpando os tweets\n",
    "\n",
    "Esta etapa é muito importante, porque a maioria dos posts acabam saindo com muita \"sujeira\", esta pode ser desde vírgulas até parênteses, que podem atrapalhar o classificador. Outra ferramenta de limpeza que usaremos será para manter somente o radical da palavra, assim vamos diminuir a variação de palavras com o mesmo radical mas complementos diferentes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Abrindo o arquivo:**<br>\n",
    "Este já possui uma classificação feita manualmente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Atribuindo o arquivo com os tweets:\n",
    "df = pd.read_excel('Tesla Model S.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_treinamento = df.drop('B1',axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...\n",
    "\n",
    "#### Vamos criar uma função para a limpeza dos tweets:\n",
    "\n",
    "Esta função será usada mais para frente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vamos criar uma função que irá substituir os caracteres que trapalham o classificador:\n",
    "def Cleaner (Tweets):\n",
    "    #Os elementos na primeira posição do replace serão substituidos por espaços:\n",
    "    Tweets = Tweets.replace(\"*\",\"\")\n",
    "    Tweets = Tweets.replace(\"!\",\"\")\n",
    "    Tweets = Tweets.replace(\"@\",\"\")\n",
    "    Tweets = Tweets.replace(\"#\",\"\")\n",
    "    Tweets = Tweets.replace(\"$\",\"\")\n",
    "    Tweets = Tweets.replace(\"%\",\"\")\n",
    "    Tweets = Tweets.replace(\"&\",\"\")\n",
    "    Tweets = Tweets.replace(\"-\",\"\")\n",
    "    Tweets = Tweets.replace(\"_\",\"\")\n",
    "    Tweets = Tweets.replace(\"+\",\"\")\n",
    "    Tweets = Tweets.replace(\"=\",\"\")\n",
    "    Tweets = Tweets.replace(\"'\",\"\")\n",
    "    Tweets = Tweets.replace(\"?\",\"\")\n",
    "    Tweets = Tweets.replace(\";\",\"\")\n",
    "    Tweets = Tweets.replace(\",\",\"\")\n",
    "    Tweets = Tweets.replace(\".\",\"\")\n",
    "    Tweets = Tweets.replace(\":\",\"\")\n",
    "    Tweets = Tweets.replace(\")\",\"\")\n",
    "    Tweets = Tweets.replace(\"(\",\"\")\n",
    "    Tweets = Tweets.replace(\"/\",\"\")\n",
    "    Tweets = Tweets.replace('\"',\"\")\n",
    "    Tweets = Tweets.replace(\"[\",\"\")\n",
    "    Tweets = Tweets.replace(']',\"\")\n",
    "    Tweets = Tweets.replace(\"\\ \",\"\")\n",
    "    \n",
    "    #Aproveitando a nossa função ja vamos separar todas as palavras:\n",
    "    Tweets = Tweets.split()\n",
    "\n",
    "    return Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COLOCAR NA DEF DE LIMPEZA: --------------------\n",
    "ps = PorterStemmer()\n",
    "\n",
    "def bla (tweets):\n",
    "    \n",
    "    post = word_tokenize(Tweet) \n",
    "    Tweets_stem = [ps.stem(word) for word in post]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Analisando quantos emails ham e spam existem no dataframe de treinamento:\n",
    "Relevante, Irrelevante = (df_treinamento[\"B1\"]).value_counts()\n",
    "Total_treinamento = df_treinamento[\"B1\"].value_counts().sum()\n",
    "print(\"O dataframe de treinamento possui:\\n\",\"HAM: {0} emails\\n\".format(ham)\n",
    "                                            ,\"SPAM: {0} emails\\n\".format(spam),\"\\nTOTAL: {0}\".format(ham+spam))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### Montando o Classificador Naive-Bayes\n",
    "\n",
    "Considerando apenas as mensagens da planilha Treinamento, ensine  seu classificador."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### Verificando a performance\n",
    "\n",
    "Agora você deve testar o seu classificador com a base de Testes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### Concluindo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aperfeiçoamento:\n",
    "\n",
    "Os trabalhos vão evoluir em conceito dependendo da quantidade de itens avançados:\n",
    "\n",
    "* Limpar: \\n, :, \", ', (, ), etc SEM remover emojis\n",
    "* Corrigir separação de espaços entre palavras e emojis ou emojis e emojis\n",
    "* Propor outras limpezas e transformações que não afetem a qualidade da informação ou classificação\n",
    "* Criar categorias intermediárias de relevância baseadas na probabilidade: ex.: muito relevante, relevante, neutro, irrelevante, muito irrelevante (3 categorias: C, mais categorias conta para B)\n",
    "* Explicar por que não posso usar o próprio classificador para gerar mais amostras de treinamento\n",
    "* Propor diferentes cenários para Naïve Bayes fora do contexto do projeto\n",
    "* Sugerir e explicar melhorias reais com indicações concretas de como implementar (indicar como fazer e indicar material de pesquisa)\n",
    "* Montar um dashboard que periodicamente realiza análise de sentimento e visualiza estes dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Referências"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Naive Bayes and Text Classification](https://arxiv.org/pdf/1410.5329.pdf)  **Mais completo**\n",
    "\n",
    "[A practical explanation of a Naive Bayes Classifier](https://monkeylearn.com/blog/practical-explanation-naive-bayes-classifier/) **Mais simples**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://www.geeksforgeeks.org/python-stemming-words-with-nltk/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
